{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def echo_log(action, text=\"\"):\n",
    "    print(\"\\033[4m\\033[1m\"+action+\"\\033[0m\\033[0m \"+text)\n",
    "\n",
    "#=======================================================================\n",
    "#   Import Modules\n",
    "#=======================================================================\n",
    "import os\n",
    "from libs import DataCleaner, Seq2SeqModel, TextSummaryWordLevelDataGenerator, CharacterLevelTokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================\n",
    "max_text_len=256 # Characters\n",
    "max_summary_len=128 # Characters\n",
    "batch_size = 60\n",
    "#======================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[1mSearching...\u001b[0m\u001b[0m for prepared data set on path: text_summary_prepared.csv\n",
      "\u001b[4m\u001b[1mExist.\u001b[0m\u001b[0m prepared dataset exist on: text_summary_prepared.csv\n",
      "\u001b[4m\u001b[1mreading dateset\u001b[0m\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "#=======================================================================\n",
    "#   Read Dataset and preform some preprocessing\n",
    "#=======================================================================\n",
    "prepared_dataset_path = 'text_summary_prepared.csv'\n",
    "echo_log(\"Searching...\", f\"for prepared data set on path: {prepared_dataset_path}\")\n",
    "if os.path.exists(prepared_dataset_path):\n",
    "    echo_log(\"Exist.\", f\"prepared dataset exist on: {prepared_dataset_path}\")\n",
    "    echo_log(\"reading dateset\")\n",
    "    df=pd.read_csv(prepared_dataset_path) #,nrows=1000\n",
    "else:\n",
    "    echo_log(\"Not Found.\", f\"prepared dataset not exist on: {prepared_dataset_path}\")\n",
    "    echo_log(\"reading default dateset\", \"on text_summary.csv\")\n",
    "    df=pd.read_csv(\"text_summary.csv\",nrows=1000) #,nrows=1000\n",
    "\n",
    "    df = df[(df['text'].str.len() < max_text_len) & (df['summary'].str.len() < max_summary_len-2)]\n",
    "\n",
    "    echo_log(\"Begin Cleaning\")\n",
    "    df.drop_duplicates(subset=['text'],inplace=True) #dropping duplicates\n",
    "    df.dropna(axis=0,inplace=True)#dropping na\n",
    "\n",
    "    echo_log(\"cleaning ......\")\n",
    "    #Preprocessing\n",
    "    df['text'] = DataCleaner.clean(df['text'], uniform_arabic_characters=True)\n",
    "    df['summary'] = DataCleaner.clean(df['summary'], remove_stop_words=False, uniform_arabic_characters=True)\n",
    "\n",
    "    df.replace('', np.nan, inplace=True)\n",
    "    df.dropna(axis=0,inplace=True)\n",
    "\n",
    "    df.to_csv(prepared_dataset_path, index=False)\n",
    "    echo_log(\"Successfully Cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[1mSearching...\u001b[0m\u001b[0m for saved tokenizer on path: ./outputs/tokenizer.json\n",
      "\u001b[4m\u001b[1mLoading tokenizer ...\u001b[0m\u001b[0m from: ./outputs/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "#=======================================================================\n",
    "#   Initialize tokenizer, split dataset\n",
    "#=======================================================================\n",
    "tokenizer_path = \"./outputs/tokenizer.json\"\n",
    "echo_log(\"Searching...\", f\"for saved tokenizer on path: {tokenizer_path}\")\n",
    "if os.path.exists(tokenizer_path):\n",
    "    echo_log(\"Loading tokenizer ...\", f\"from: {tokenizer_path}\")\n",
    "    tokenizer = CharacterLevelTokenizer.from_json(tokenizer_path)\n",
    "else:\n",
    "    echo_log(\"Init tokenizer and fit on texts ...\")\n",
    "    #prepare a tokenizer for reviews on training data\n",
    "    tokenizer = CharacterLevelTokenizer() \n",
    "    tokenizer.fit_on_texts(list(df['summary']) + list(df['text']))\n",
    "    echo_log(\"Number of Vocab:\",str(len(tokenizer.token_index)))\n",
    "\n",
    "    # Save tokenizer to a JSON file\n",
    "    tokenizer_json = tokenizer.to_json(tokenizer_path)\n",
    "    echo_log(\"Saving tokenizer ...\", f\"json_file on => {tokenizer_path}\")\n",
    "\n",
    "    echo_log(\"Number of Vocab:\",str(len(tokenizer.token_index)))\n",
    "\n",
    "\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=False)\n",
    "\n",
    "# Using data generator\n",
    "train_gen = TextSummaryWordLevelDataGenerator(data_frame=pd.DataFrame({\"text\": x_tr, \"summary\": y_tr}), tokenizer=tokenizer, \n",
    "                                               max_text_len=max_text_len, max_summary_len=max_summary_len, batch_size=batch_size)\n",
    "valid_gen = TextSummaryWordLevelDataGenerator(data_frame=pd.DataFrame({\"text\": x_val, \"summary\": y_val}), tokenizer=tokenizer,\n",
    "                                              max_text_len=max_text_len, max_summary_len=max_summary_len, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      " [[3, 74, 71, 49, 5, 50, 49, 71, 66, 49, 71, 72], [1]]\n"
     ]
    }
   ],
   "source": [
    "print(15*\"==\"+\"\\n\",tokenizer.texts_to_sequences([\"أهلا بالعالم\", \"<SOS>\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Moatasem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:74: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Epoch 1/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:19\u001b[0m 86s/step - loss: 4.3811"
     ]
    }
   ],
   "source": [
    "# STEPS_PER_EPOCH = len(df.text) / batch_size\n",
    "# SAVE_PERIOD = 25\n",
    "# save_freq= int(SAVE_PERIOD * STEPS_PER_EPOCH)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=5)\n",
    "#=======================================================================\n",
    "#   Build, Compile, Train Model\n",
    "#=======================================================================\n",
    "vocab_size = len(tokenizer.token_index) + 1\n",
    "\n",
    "model = Seq2SeqModel(input_vocab_size=vocab_size, output_vocab_size=vocab_size,\n",
    "                    max_input_length=max_text_len, max_output_length=max_summary_len, latent_dim=300, embedding_dim=100);\n",
    "model.build_model()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit_using_data_generator(train_generator=train_gen, checkpoints_saving_path='outputs/checkpoints',\n",
    "                                validation_generator=valid_gen, callbacks=[es], epochs=3000)\n",
    "model.save('outputs/s2s_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
